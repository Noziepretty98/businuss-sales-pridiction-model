{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing:\n",
    "1: Import Libraries and Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\myproject\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Print the current working directory to ensure we are in the correct folder\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "   id        date  store_nbr        family  sales  onpromotion\n",
      "0   0  2013-01-01          1    AUTOMOTIVE    0.0            0\n",
      "1   1  2013-01-01          1     BABY CARE    0.0            0\n",
      "2   2  2013-01-01          1        BEAUTY    0.0            0\n",
      "3   3  2013-01-01          1     BEVERAGES    0.0            0\n",
      "4   4  2013-01-01          1         BOOKS    0.0            0\n",
      "5   5  2013-01-01          1  BREAD/BAKERY    0.0            0\n",
      "6   6  2013-01-01          1   CELEBRATION    0.0            0\n",
      "7   7  2013-01-01          1      CLEANING    0.0            0\n",
      "8   8  2013-01-01          1         DAIRY    0.0            0\n",
      "9   9  2013-01-01          1          DELI    0.0            0\n",
      "\n",
      "Transactions Dataset:\n",
      "         date  store_nbr  transactions\n",
      "0  2013-01-01         25           770\n",
      "1  2013-01-02          1          2111\n",
      "2  2013-01-02          2          2358\n",
      "3  2013-01-02          3          3487\n",
      "4  2013-01-02          4          1922\n",
      "5  2013-01-02          5          1903\n",
      "6  2013-01-02          6          2143\n",
      "7  2013-01-02          7          1874\n",
      "8  2013-01-02          8          3250\n",
      "9  2013-01-02          9          2940\n",
      "\n",
      "Test Dataset:\n",
      "        id        date  store_nbr        family  onpromotion\n",
      "0  3000888  2017-08-16          1    AUTOMOTIVE            0\n",
      "1  3000889  2017-08-16          1     BABY CARE            0\n",
      "2  3000890  2017-08-16          1        BEAUTY            2\n",
      "3  3000891  2017-08-16          1     BEVERAGES           20\n",
      "4  3000892  2017-08-16          1         BOOKS            0\n",
      "5  3000893  2017-08-16          1  BREAD/BAKERY           12\n",
      "6  3000894  2017-08-16          1   CELEBRATION            0\n",
      "7  3000895  2017-08-16          1      CLEANING           25\n",
      "8  3000896  2017-08-16          1         DAIRY           45\n",
      "9  3000897  2017-08-16          1          DELI           18\n",
      "\n",
      "US Retail Sales Dataset:\n",
      "     Month  Clothing  Appliances  FoodAndBeverage  Automobiles  \\\n",
      "0  1992-01      6938        3657            29589        26788   \n",
      "1  1992-02      7524        3490            28570        28203   \n",
      "2  1992-03      8475        3669            29682        31684   \n",
      "3  1992-04      9401        3527            30228        32547   \n",
      "4  1992-05      9558        3571            31677        32883   \n",
      "5  1992-06      9182        3788            30769        34667   \n",
      "6  1992-07      9103        3930            32402        34077   \n",
      "7  1992-08     10513        3817            31469        31809   \n",
      "8  1992-09      9573        3795            30162        32337   \n",
      "9  1992-10     10254        4014            31407        32947   \n",
      "\n",
      "   GeneralMerchandise  BuildingMaterials  \n",
      "0               14996               8964  \n",
      "1               16015               9023  \n",
      "2               17984              10608  \n",
      "3               18872              11630  \n",
      "4               20037              12327  \n",
      "5               18812              12219  \n",
      "6               18575              11658  \n",
      "7               20518              11219  \n",
      "8               18700              11309  \n",
      "9               20970              11605  \n"
     ]
    }
   ],
   "source": [
    "# Load datasets from specified file paths\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(r'C:\\myproject\\env\\train.csv\\train.csv')\n",
    "transactions_df = pd.read_csv(r'C:\\myproject\\env\\transactions.csv\\transactions.csv')\n",
    "test_df = pd.read_csv(r'C:\\myproject\\env\\test.csv')  \n",
    "us_retail_sales_df = pd.read_csv(r'C:\\myproject\\env\\us-retail-sales.csv')\n",
    "\n",
    "\n",
    "# Preview the first few rows of each dataset to understand their structure\n",
    "print(\"\\nTrain Dataset:\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "print(\"\\nTransactions Dataset:\")\n",
    "print(transactions_df.head(10))\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df.head(10))\n",
    "\n",
    "print(\"\\nUS Retail Sales Dataset:\")\n",
    "print(us_retail_sales_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Check for Missing Values in the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in Train dataset:\n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "family         0\n",
      "sales          0\n",
      "onpromotion    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Transactions dataset:\n",
      "date            0\n",
      "store_nbr       0\n",
      "transactions    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test dataset:\n",
      "id             0\n",
      "date           0\n",
      "store_nbr      0\n",
      "family         0\n",
      "onpromotion    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in US Retail Sales dataset:\n",
      "Month                 0\n",
      "Clothing              0\n",
      "Appliances            0\n",
      "FoodAndBeverage       0\n",
      "Automobiles           0\n",
      "GeneralMerchandise    0\n",
      "BuildingMaterials     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each dataset to identify any potential issues\n",
    "print(\"\\nMissing values in Train dataset:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Transactions dataset:\")\n",
    "print(transactions_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Test dataset:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in US Retail Sales dataset:\")\n",
    "print(us_retail_sales_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Convert Date Columns to Datetime Format\n",
    "Since date columns are often in string format, converting them to datetime ensures proper handling of time-based operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' columns to datetime format in the relevant datasets\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique combinations in Train Dataset: (90936, 2)\n",
      "Unique combinations in Transactions Dataset: (83488, 2)\n",
      "Combinations in train_df but not in transactions_df: 7448 rows\n",
      "          date  store_nbr\n",
      "0   2013-01-01          1\n",
      "33  2013-01-01         10\n",
      "66  2013-01-01         11\n",
      "99  2013-01-01         12\n",
      "132 2013-01-01         13\n"
     ]
    }
   ],
   "source": [
    "# Check unique combinations of store_nbr and date in both datasets\n",
    "train_combination = train_df[['date', 'store_nbr']].drop_duplicates()\n",
    "transactions_combination = transactions_df[['date', 'store_nbr']].drop_duplicates()\n",
    "\n",
    "# Print the number of unique combinations in each dataset\n",
    "print(\"Unique combinations in Train Dataset:\", train_combination.shape)\n",
    "print(\"Unique combinations in Transactions Dataset:\", transactions_combination.shape)\n",
    "\n",
    "# Find combinations that are in train_df but not in transactions_df\n",
    "missing_combinations = train_combination[~train_combination.apply(tuple, 1).isin(transactions_combination.apply(tuple, 1))]\n",
    "print(f\"Combinations in train_df but not in transactions_df: {missing_combinations.shape[0]} rows\")\n",
    "print(missing_combinations.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Date Range: (Timestamp('2013-01-01 00:00:00'), Timestamp('2017-08-15 00:00:00'))\n",
      "Transactions Dataset Date Range: (Timestamp('2013-01-01 00:00:00'), Timestamp('2017-08-15 00:00:00'))\n"
     ]
    }
   ],
   "source": [
    "# Check the date range in both datasets\n",
    "train_date_range = train_df['date'].min(), train_df['date'].max()\n",
    "transactions_date_range = transactions_df['date'].min(), transactions_df['date'].max()\n",
    "\n",
    "print(f\"Train Dataset Date Range: {train_date_range}\")\n",
    "print(f\"Transactions Dataset Date Range: {transactions_date_range}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Merge Datasets\n",
    "We will now merge the datasets to combine relevant information from the different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged Dataset Preview:\n",
      "   id       date  store_nbr      family  sales  onpromotion  transactions  \\\n",
      "0   0 2013-01-01          1  AUTOMOTIVE    0.0            0           NaN   \n",
      "1   1 2013-01-01          1   BABY CARE    0.0            0           NaN   \n",
      "2   2 2013-01-01          1      BEAUTY    0.0            0           NaN   \n",
      "3   3 2013-01-01          1   BEVERAGES    0.0            0           NaN   \n",
      "4   4 2013-01-01          1       BOOKS    0.0            0           NaN   \n",
      "\n",
      "   Clothing  Appliances  FoodAndBeverage  Automobiles  GeneralMerchandise  \\\n",
      "0   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "1   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "2   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "3   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "4   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "\n",
      "   BuildingMaterials  \n",
      "0            19284.0  \n",
      "1            19284.0  \n",
      "2            19284.0  \n",
      "3            19284.0  \n",
      "4            19284.0  \n"
     ]
    }
   ],
   "source": [
    "# Merge 'train_df' with 'transactions_df' on the 'date' and 'store_nbr' columns\n",
    "merged_df = pd.merge(train_df, transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Convert 'Month' column in 'us_retail_sales_df' to datetime format for merging\n",
    "us_retail_sales_df['Month'] = pd.to_datetime(us_retail_sales_df['Month'], format='%Y-%m')\n",
    "\n",
    "# Merge the 'us_retail_sales_df' with 'merged_df' based on the 'date' column\n",
    "merged_df = pd.merge(merged_df, us_retail_sales_df, left_on='date', right_on='Month', how='left')\n",
    "\n",
    "# Drop the redundant 'Month' column after merging\n",
    "merged_df.drop(columns=['Month'], inplace=True)\n",
    "\n",
    "# Preview the resulting merged dataset to ensure everything is correct\n",
    "print(\"\\nMerged Dataset Preview:\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Handle Missing Values in the 'transactions' Column\n",
    "To ensure that we handle missing data in the merged dataset, especially for the transactions column, we'll apply forward and backward filling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an inner join if you want only matching rows\n",
    "merged_df_inner = pd.merge(train_df, transactions_df, on=[\"date\", \"store_nbr\"], how=\"inner\")\n",
    "\n",
    "# Perform a left join if you want to keep all rows from train_df and add missing transactions as NaN\n",
    "merged_df_left = pd.merge(train_df, transactions_df, on=[\"date\", \"store_nbr\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_left['transactions'] = merged_df_left['transactions'].fillna(merged_df_left['transactions'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "date                        0\n",
       "store_nbr                   0\n",
       "family                      0\n",
       "sales                       0\n",
       "onpromotion                 0\n",
       "transactions           245784\n",
       "Clothing              2901096\n",
       "Appliances            2901096\n",
       "FoodAndBeverage       2901096\n",
       "Automobiles           2901096\n",
       "GeneralMerchandise    2901096\n",
       "BuildingMaterials     2901096\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_left['transactions'] = merged_df_left['transactions'].fillna(merged_df_left['transactions'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              0\n",
      "date            0\n",
      "store_nbr       0\n",
      "family          0\n",
      "sales           0\n",
      "onpromotion     0\n",
      "transactions    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df_left.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Final Dataset Preview\n",
    "Once the merging and missing value handling are done, let's preview the final cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned and Merged Dataset Preview:\n",
      "   id       date  store_nbr        family  sales  onpromotion  transactions  \\\n",
      "0   0 2013-01-01          1    AUTOMOTIVE    0.0            0           NaN   \n",
      "1   1 2013-01-01          1     BABY CARE    0.0            0           NaN   \n",
      "2   2 2013-01-01          1        BEAUTY    0.0            0           NaN   \n",
      "3   3 2013-01-01          1     BEVERAGES    0.0            0           NaN   \n",
      "4   4 2013-01-01          1         BOOKS    0.0            0           NaN   \n",
      "5   5 2013-01-01          1  BREAD/BAKERY    0.0            0           NaN   \n",
      "6   6 2013-01-01          1   CELEBRATION    0.0            0           NaN   \n",
      "7   7 2013-01-01          1      CLEANING    0.0            0           NaN   \n",
      "8   8 2013-01-01          1         DAIRY    0.0            0           NaN   \n",
      "9   9 2013-01-01          1          DELI    0.0            0           NaN   \n",
      "\n",
      "   Clothing  Appliances  FoodAndBeverage  Automobiles  GeneralMerchandise  \\\n",
      "0   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "1   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "2   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "3   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "4   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "5   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "6   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "7   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "8   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "9   15165.0      8168.0          51652.0      62999.0             46639.0   \n",
      "\n",
      "   BuildingMaterials  \n",
      "0            19284.0  \n",
      "1            19284.0  \n",
      "2            19284.0  \n",
      "3            19284.0  \n",
      "4            19284.0  \n",
      "5            19284.0  \n",
      "6            19284.0  \n",
      "7            19284.0  \n",
      "8            19284.0  \n",
      "9            19284.0  \n"
     ]
    }
   ],
   "source": [
    "# Final preview of the cleaned and merged dataset\n",
    "print(\"\\nCleaned and Merged Dataset Preview:\")\n",
    "print(merged_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached scikit_learn-1.5.2-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in .\\env\\lib\\site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.5.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.14.1-cp313-cp313-win_amd64.whl (44.5 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1480271085.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[117], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    sklearn.preprocessing import StandardScaler\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merged_df_left[['Clothing', 'Appliances', 'FoodAndBeverage', 'Automobiles', 'GeneralMerchandise', 'BuildingMaterials']] = scaler.fit_transform(merged_df_left[['Clothing', 'Appliances', 'FoodAndBeverage', 'Automobiles', 'GeneralMerchandise', 'BuildingMaterials']])\n",
    " sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merged_df_left[['Clothing', 'Appliances', 'FoodAndBeverage', 'Automobiles', 'GeneralMerchandise', 'BuildingMaterials']] = scaler.fit_transform(merged_df_left[['Clothing', 'Appliances', 'FoodAndBeverage', 'Automobiles', 'GeneralMerchandise', 'BuildingMaterials']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
